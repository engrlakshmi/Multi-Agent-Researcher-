# Multi-Agent-Researcher
# ğŸ” Streamlit Research Summarizer (DuckDuckGo + Ollama)

A **Streamlit-based research tool** that:
1. Searches the web using **DuckDuckGo** (via LangChain tools)
2. Cleans up the query
3. Uses an **Ollama LLM** to refine and summarize the results in real time

---

## ğŸ“Œ Features
- **Live Search**: Uses DuckDuckGo to get the latest information
- **Query Cleaning**: Removes unnecessary spaces for better search accuracy
- **Single LLM Call**: Combines refinement and summarization in one pass
- **Streaming Output**: Displays AI-generated summaries as they are produced
- **Caching**: Avoids repeated searches for the same query

---

## ğŸ›  Requirements
- **Python 3.9+**
- **Ollama** installed locally (for the `qwen3:8b` model) â†’ [Ollama Installation Guide](https://ollama.com/download)
- **Docker** (if running in a container)

---

## ğŸ“‚ Project Structure
.
â”œâ”€â”€ app.py # Streamlit app
â”œâ”€â”€ Dockerfile # Docker setup
â””â”€â”€ requirements.txt # Python dependencies

## ğŸ“¦ Installation (Local)

1ï¸âƒ£ Clone the repo:
git clone https://github.com/yourusername/research-summarizer.git
cd research-summarizer
2ï¸âƒ£ Install dependencies:
pip install -r requirements.txt
3ï¸âƒ£ Start Ollama locally:
ollama serve
ollama pull qwen3:8b
4ï¸âƒ£ Run the app:
streamlit run app.py
ğŸ³ Running with Docker
1ï¸âƒ£ Build the Docker image:
docker build -t research-summarizer .
2ï¸âƒ£ Run the container:
docker run -p 8501:8501 research-summarizer
3ï¸âƒ£ Access in browser:
http://localhost:8501
Note: The container must be able to connect to your local Ollama server, so you may need to expose its API port (11434).

ğŸ“œ Example Usage
Enter:
Latest AI news August 2025
The app:
1. Cleans the query
2. Searches DuckDuckGo
3. Summarizes results using qwen3:8b
4. Displays a 200-word concise summary
